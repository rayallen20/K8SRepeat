# 附录1-K8S集群搭建

## PART1. 规划

### 1.1 节点规划

|IP地址|主机名|角色|配置|
|:-:|:-:|:-:|:-:|
|192.168.1.56|longinus-master-1|Master|2C 4G 20G|
|192.168.1.57|longinus-node-1|Worker|4C 8G 20G|
|192.168.1.58|longinus-node-2|Worker|4C 8G 20G|
|192.168.1.59|longinus-node-3|Worker|4C 8G 20G|

### 1.2 软件版本规划

|软件包|版本|安装性质|获取渠道|贡献者|
|:-:|:-:|:-:|:-:|:-:|
|docker-ce|24.0.7|dub|阿里云|docker|
|docker-ce-cli|24.0.7|dub|阿里云|docker|
|containerd.io|1.6.26|dub|阿里云|docker|
|kubelet|1.28.2|dub|阿里云|kubernetes|
|kubeadm|1.28.2|dub|阿里云|kubernetes|
|kubectl|1.28.2|dub|阿里云|kubernetes|
|kube-apiserver|1.28.2|pod|阿里云|kubernetes|
|kube-controller-manager|1.28.2|pod|阿里云|kubernetes|
|kube-scheduler|1.28.2|pod|阿里云|kubernetes|
|kube-proxy|1.28.2|pod|阿里云|kubernetes|
|pause|3.9|pod|阿里云|kubernetes|
|coredns|1.10.1|pod|阿里云|coredns|
|etcd|3.5.9-0|pod|阿里云|coreos|
|flannel|cniVersion 0.3.1|pod|github|coreos|

## 1.3 安装规划

### 1.3.1 容器选择

各节点都安装`docker-ce`、`docker-ce-cli`、`containerd.io`,使用`Containerd`作为容器运行时,和`kubelet`交互

Cgroup管理器,k8s默认是systemd,需要将Containerd的Cgroup管理器也修改为systemd(默认是cgroupfs)

### 1.3.2 CNI网络插件

使用谷歌嫡系,CoreOS提供的flannel作为网络插件

### 1.3.3 安装步骤

各节点都安装`docker-ce`、`docker-ce-cli`、`containerd.io`软件包,启动`docker.socket`、`docker.service`、`containerd.service`服务

各节点都安装`kubelet`、`kubeadm`、`kubectl`软件包,都启动`kubelet.service`服务

`longinus-master-1`节点使用`kubeadm`创建集群,且`longinus-master-1`节点作为master节点;集群创建后马上安装flannel建立私网

集群创建后,将`longinus-node-1`、`longinus-node-2`、`longinus-node-3`节点加入集群作为worker节点

运行一个比helloworld稍微复杂一些的demo,验证应用pod可以启动并提供服务

## PART2. 操作系统准备工作

### 2.1 ubuntu前置检查

#### 2.1.1 关闭ufw

```
root@longinus-master-1:~# ufw status
Status: inactive
```

#### 2.1.2 时钟同步

- 调整时区

```
root@longinus-master-1:~# dpkg-reconfigure tzdata
```

选`Asia`,再选`Shanghai`即可

- 确认时钟同步

```
root@longinus-master-1:~# timedatectl status
               Local time: Tue 2024-01-16 09:44:38 CST
           Universal time: Tue 2024-01-16 01:44:38 UTC
                 RTC time: Tue 2024-01-16 01:44:37
                Time zone: Asia/Shanghai (CST, +0800)
System clock synchronized: yes
              NTP service: active
          RTC in local TZ: no
```

`System clock synchronized: yes`即为开启时钟同步服务

#### 2.1.3 配置apt主源

```
root@longinus-node-2:~# apt update
Hit:1 http://us.archive.ubuntu.com/ubuntu jammy InRelease
Hit:2 http://us.archive.ubuntu.com/ubuntu jammy-updates InRelease
Hit:3 http://us.archive.ubuntu.com/ubuntu jammy-backports InRelease
Hit:4 http://us.archive.ubuntu.com/ubuntu jammy-security InRelease
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
All packages are up to date.
```

#### 2.1.4 加载`br_netfilter`模块

- 加载`br_netfilter`模块

```
root@longinus-master-1:~# modprobe br_netfilter
root@longinus-master-1:~# echo 1 | sudo tee /proc/sys/net/bridge/bridge-nf-call-iptables
1
```

- 检查

```
root@longinus-master-1:~# lsmod|grep br_netfilt
br_netfilter           32768  0
bridge                307200  1 br_netfilter
```

#### 2.1.5 启用IP转发

- 启用IP转发

```
root@longinus-master-1:~# echo 1 | sudo tee /proc/sys/net/ipv4/ip_forward
1
```

- 检查

```
root@longinus-master-1:~# cat /proc/sys/net/ipv4/ip_forward
1
```

#### 2.1.6 启用overlay模块

- 启用

```
root@longinus-master-1:~# modprobe overlay
```

- 设置模块在开机启动时加载

```
root@longinus-master-1:~# vim /etc/modules-load.d/modules.conf
```

在该文件中添加一行,其内容为`overlay`

- 检查

```
root@longinus-master-1:~# lsmod | grep overlay
overlay               151552  0
```

#### 2.1.7 安装ipvsadm和ipset

- 安装

```
root@longinus-master-1:~# apt install -y ipset ipvsadm
```

- 检查

```
root@longinus-master-1:~# dpkg -l ipset ipvsadm
Desired=Unknown/Install/Remove/Purge/Hold
| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend
|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)
||/ Name           Version        Architecture Description
+++-==============-==============-============-================================>
ii  ipset          7.15-1build1   amd64        administration tool for kernel I>
ii  ipvsadm        1:1.31-1build2 amd64        Linux Virtual Server support pro>
```

注意:此处的`ii`表示安装成功

### 2.2 关闭swap

```
root@longinus-master-1:~# swapoff -a
root@longinus-master-1:~# vim /etc/fstab 
root@longinus-master-1:~# swapon --show
root@longinus-master-1:~# 
```

在`/etc/fstab`中注释掉带有`swap`的行即可

正确结果应为执行`swapon --show`命令后无任何返回结果

### 2.3 配置docker和k8s的apt源

#### 2.3.1 配置docker的apt源

```
root@longinus-master-1:~# curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -
```

这一步会有个Warning级别的错误,正常现象

```
root@longinus-master-1:~# add-apt-repository "deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"
```

这一步需要按个回车

#### 2.3.2 配置kubernetes的apt源

```
root@longinus-master-1:~# curl -fsSL https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -
```

```
root@longinus-master-1:~# add-apt-repository "deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main"
```

### 2.4 绑定host表

```
root@longinus-master-1:~# vim /etc/hosts
root@longinus-master-1:~# cat /etc/hosts
```

```
127.0.0.1 localhost
127.0.1.1 template-1

# The following lines are desirable for IPv6 capable hosts
::1     ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters

# master1节点公网IP
192.168.1.56 k8s-master01.longinus.com  k8s-master01.longinus kubeapi.longinus.com

# worker1节点公网IP
192.168.1.57 k8s-node01.longinus.com k8s-node01.longinus

# worker2节点公网IP
192.168.1.58 k8s-node02.longinus.com k8s-node02.longinus

# worker3节点公网IP
192.168.1.59 k8s-node03.longinus.com k8s-node03.longinus
```

## PART3. 安装docker

### 3.1 安装docker

- 安装

```
root@longinus-master-1:~# apt install -y docker-ce docker-ce-cli containerd.io
```

- 检查

```
root@longinus-master-1:~# apt list --installed | grep -i -E 'docker|containerd'

WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

containerd.io/jammy,now 1.6.27-1 amd64 [installed]
docker-buildx-plugin/jammy,now 0.11.2-1~ubuntu.22.04~jammy amd64 [installed,automatic]
docker-ce-cli/jammy,now 5:24.0.7-1~ubuntu.22.04~jammy amd64 [installed]
docker-ce-rootless-extras/jammy,now 5:24.0.7-1~ubuntu.22.04~jammy amd64 [installed,automatic]
docker-ce/jammy,now 5:24.0.7-1~ubuntu.22.04~jammy amd64 [installed]
docker-compose-plugin/jammy,now 2.21.0-1~ubuntu.22.04~jammy amd64 [installed,automatic]
```

这里有个警告没关系

### 3.2 启动docker相关服务

#### 3.2.1 验证是否开机自启

```
root@longinus-master-1:~# systemctl list-unit-files | grep -E 'docker|containerd'
containerd.service                         enabled         enabled
docker.service                             enabled         enabled
docker.socket                              enabled         enabled
```

#### 3.2.2 验证服务是否启动

```
root@longinus-master-1:~# systemctl status containerd.service
root@longinus-master-1:~# systemctl status docker.service
root@longinus-master-1:~# systemctl status docker.socket
```

3个服务应该都是running状态

#### 3.2.3 查看docker版本

```
root@longinus-master-1:~# docker version
```

### 3.3 配置containerd

#### 3.3.1 备份原配置并使用默认配置替换配置文件

- 备份原配置

```
root@longinus-master-1:~# cp /etc/containerd/config.toml /etc/containerd/config.toml.ori
```

- 使用默认配置替换配置文件

```
root@longinus-master-1:~# containerd config default > /etc/containerd/config.toml
```

#### 3.3.2 设置Containerd的Cgroup设为systemd

```
root@longinus-master-1:~# vim /etc/containerd/config.toml
```

第139行:

原文:`SystemdCgroup = false`

修改为:`SystemdCgroup = true`

#### 3.3.3 pause镜像路径改为国内源

```
root@longinus-master-1:~# vim /etc/containerd/config.toml
```

第61行:

原文:`sandbox_image = "registry.k8s.io/pause:3.6"`

修改为:`sandbox_image = "registry.aliyuncs.com/google_containers/pause:3.9"`

#### 3.3.4 重启containerd并查看服务状态

```
root@longinus-master-1:~# systemctl restart containerd.service
root@longinus-master-1:~# systemctl status containerd.service
```

## PART4. 安装Kubernetes

### 4.1 安装Kubernetes软件包

```
root@longinus-master-1:~# apt install -y kubelet kubeadm kubectl
```

注:这一步如果遇到错误

```
E: Unable to locate package kubelet
E: Unable to locate package kubeadm
E: Unable to locate package kubectl
```

则重新执行2.3.2小节,然后执行`apt-get update`即可

### 4.2 确认kubelet服务状态

```
root@longinus-master-1:~# systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor prese>
    Drop-In: /etc/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: activating (auto-restart) (Result: exit-code) since Tue 2024-01-16>
       Docs: https://kubernetes.io/docs/home/
    Process: 7108 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_>
   Main PID: 7108 (code=exited, status=1/FAILURE)
        CPU: 61ms

Jan 16 11:02:01 longinus-master-1 systemd[1]: kubelet.service: Main process exi>
Jan 16 11:02:01 longinus-master-1 systemd[1]: kubelet.service: Failed with resu>
lines 1-12/12 (END)...skipping...
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /etc/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: activating (auto-restart) (Result: exit-code) since Tue 2024-01-16 11:02:01 CST; 1s ago
       Docs: https://kubernetes.io/docs/home/
    Process: 7108 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=1/FAILURE)
   Main PID: 7108 (code=exited, status=1/FAILURE)
        CPU: 61ms

Jan 16 11:02:01 longinus-master-1 systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Jan 16 11:02:01 longinus-master-1 systemd[1]: kubelet.service: Failed with result 'exit-code'.
```

可以看到此时kubelet状态为loaded.这是正常的.因为kubelet服务成功启动的先决条件是需要kubelet的配置文件,所在目录`/var/lib/kubelet`还没有建立

查看日志:

```
root@longinus-master-1:~# journalctl -xeu kubelet
```

可以看到:

```
Jan 16 11:05:16 longinus-master-1 kubelet[7227]: E0116 11:05:16.565328    7227 run.go:74] "command failed" err="failed to load kubelet config file, path: /var/lib/kubelet/config.yaml, error: failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to>
Jan 16 11:05:16 longinus-master-1 systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
```

## PART5. K8S master建立单机集群

### 5.1 拉取所需镜像

- 列出所需镜像

```
root@longinus-master-1:~# kubeadm config images list
I0116 11:07:18.093407    7309 version.go:256] remote version is much newer: v1.29.0; falling back to: stable-1.28
registry.k8s.io/kube-apiserver:v1.28.5
registry.k8s.io/kube-controller-manager:v1.28.5
registry.k8s.io/kube-scheduler:v1.28.5
registry.k8s.io/kube-proxy:v1.28.5
registry.k8s.io/pause:3.9
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
```

- 拉取镜像

```
root@longinus-master-1:~# kubeadm config images pull
I0116 11:08:28.342502    7361 version.go:256] remote version is much newer: v1.29.0; falling back to: stable-1.28
[config/images] Pulled registry.k8s.io/kube-apiserver:v1.28.5
[config/images] Pulled registry.k8s.io/kube-controller-manager:v1.28.5
[config/images] Pulled registry.k8s.io/kube-scheduler:v1.28.5
[config/images] Pulled registry.k8s.io/kube-proxy:v1.28.5
[config/images] Pulled registry.k8s.io/pause:3.9
[config/images] Pulled registry.k8s.io/etcd:3.5.9-0
[config/images] Pulled registry.k8s.io/coredns/coredns:v1.10.1
```

这里注意我们拉取镜像的版本,初始化集群时的版本要和镜像的版本相同

### 5.2 Kubernetes初始化

#### 5.2.1 初始化

```
root@longinus-master-1:~# kubeadm init \
--control-plane-endpoint="kubeapi.longinus.com" \
--kubernetes-version=v1.28.5 \
--pod-network-cidr=10.244.0.0/16 \
--service-cidr=10.96.0.0/12 \
--token-ttl=0 \
--cri-socket unix:///run/containerd/containerd.sock \
--upload-certs
```

#### 5.2.2 选项解释

- `--image-repository`:指定要使用的镜像仓库,默认为`registry.k8s.io`
- `--kubernetes-version`:kubernetes程序组件的版本号,它必须要与安装的kubelet程序包的版本号相同
- `--control-plane-endpoint`:控制平面的固定访问端点,可以是IP地址或DNS名称,会被用于集群管理员及集群组件的kubeconfig配置文件的API Server的访问地址;单控制平面部署时可以不使用该选项
- `--pod-network-cidr`:Pod网络的地址范围,其值为CIDR格式的网络地址,通常,Flannel网络插件的默认为`10.244.0.0/16`;`Project Calico`插件的默认值为`192.168.0.0/16`
- `--service-cidr`:Service的网络地址范围,其值为CIDR格式的网络地址,默认为`10.96.0.0/12`;通常,仅Flannel一类的网络插件需要手动指定该地址
- `-apiserver-advertise-address`:apiserver通告给其他组件的IP地址,一般应该为Master节点的用于集群内部通信的IP地址,`0.0.0.0`表示节点上所有可用地址
- `--token-ttl`:共享令牌(token)的过期时长,默认为24小时,0表示永不过期;为防止不安全存储等原因导致的令牌泄露危及集群安全,建议为其设定过期时长.
	- 未设定该选项时,在token过期后,若期望再向集群中加入其它节点,可以使用如下命令重新创建token,并生成节点加入命令
	- `kubeadm token create --print-join-command`

#### 5.2.3 后续操作

成功初始化后会提示后续操作:

```
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join kubeapi.longinus.com:6443 --token yczsfl.oz53z6pdlzg6l8of \
	--discovery-token-ca-cert-hash sha256:cad8ad0e1f0553a6989cc9a9a73ffee219b714fd84ed6c1ce68f768ff18b733f \
	--control-plane --certificate-key 6351ad49af82922a578621bf4d40dc17abfb2c218d041d2905bc97067771143d

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join kubeapi.longinus.com:6443 --token yczsfl.oz53z6pdlzg6l8of \
	--discovery-token-ca-cert-hash sha256:cad8ad0e1f0553a6989cc9a9a73ffee219b714fd84ed6c1ce68f768ff18b733f 
```

#### 5.2.4 检查

检查6443端口(kube API Server)是否被监听

```
root@longinus-master-1:~# lsof -i :6443
COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
kube-cont 8170 root    8u  IPv4 155311      0t0  TCP k8s-master01.longinus.com:58406->k8s-master01.longinus.com:6443 (ESTABLISHED)
kube-sche 8179 root    7u  IPv4 156409      0t0  TCP k8s-master01.longinus.com:58396->k8s-master01.longinus.com:6443 (ESTABLISHED)
kube-sche 8179 root    8u  IPv4 156571      0t0  TCP k8s-master01.longinus.com:37666->k8s-master01.longinus.com:6443 (ESTABLISHED)
kube-apis 8207 root    3u  IPv6 155276      0t0  TCP *:6443 (LISTEN)
kube-apis 8207 root   68u  IPv6 155385      0t0  TCP k8s-master01.longinus.com:6443->k8s-master01.longinus.com:37666 (ESTABLISHED)
kube-apis 8207 root   70u  IPv6 155370      0t0  TCP k8s-master01.longinus.com:6443->k8s-master01.longinus.com:58396 (ESTABLISHED)
kube-apis 8207 root   71u  IPv6 155371      0t0  TCP k8s-master01.longinus.com:6443->k8s-master01.longinus.com:58406 (ESTABLISHED)
kube-apis 8207 root   72u  IPv6 156908      0t0  TCP k8s-master01.longinus.com:6443->k8s-master01.longinus.com:37680 (ESTABLISHED)
kube-apis 8207 root   85u  IPv6 156562      0t0  TCP ip6-localhost:58480->ip6-localhost:6443 (ESTABLISHED)
kube-apis 8207 root   92u  IPv6 156569      0t0  TCP ip6-localhost:6443->ip6-localhost:58480 (ESTABLISHED)
kubelet   8296 root   15u  IPv4 156907      0t0  TCP k8s-master01.longinus.com:37680->k8s-master01.longinus.com:6443 (ESTABLISHED)
```

#### 5.2.5 配置kubelet

kubectl是kube-apiserver的命令行客户端程序,实现了除系统部署之外的几乎全部的管理操作,是kubernetes管理员使用最多的命令之一.kubectl需经由API server认证及授权后方能执行相应的管理操作,kubeadm部署的集群为其生成了一个具有管理员权限的认证配置文件`/etc/kubernetes/admin.conf`,它可由kubectl通过默认的`$HOME/.kube/config`的路径进行加载.当然,用户也可在kubectl命令上使用`--kubeconfig`选项指定一个别的位置.

下面复制认证为Kubernetes系统管理员的配置文件至目标用户(例如当前用户root)的家目录下:

```
root@longinus-master-1:~# mkdir ~/.kube
root@longinus-master-1:~# cp /etc/kubernetes/admin.conf  ~/.kube/config
root@longinus-master-1:~# ll ~/.kube/config
-rw------- 1 root root 5652 Jan 16 11:22 /root/.kube/config
```

### 5.3 安装CNI网络插件

此处我们使用Flannel作为集群的网络插件

#### 5.3.1 下载Pod的配置文件

```
root@longinus-master-1:~# wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
```

#### 5.3.2 检查配置文件

配置文件中的`net-conf.json`段需要与Kubernetes初始化时的`pod-network-cidr`配置的网段一致:

```
root@longinus-master-1:~# vim kube-flannel.yml
```

```yaml
  net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
```

#### 5.3.3 安装网络插件

```
root@longinus-master-1:~# kubectl apply -f kube-flannel.yml
namespace/kube-flannel created
serviceaccount/flannel created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created
```

#### 5.3.4 查看集群网络状态

##### 5.3.4.1 系统级Pod状态

```
root@longinus-master-1:~# kubectl get pod --all-namespaces
NAMESPACE      NAME                                        READY   STATUS    RESTARTS   AGE
kube-flannel   kube-flannel-ds-2clkz                       1/1     Running   0          44s
kube-system    coredns-5dd5756b68-pqfhw                    1/1     Running   0          12m
kube-system    coredns-5dd5756b68-vjd7k                    1/1     Running   0          12m
kube-system    etcd-longinus-master-1                      1/1     Running   0          13m
kube-system    kube-apiserver-longinus-master-1            1/1     Running   0          13m
kube-system    kube-controller-manager-longinus-master-1   1/1     Running   0          13m
kube-system    kube-proxy-j4bkt                            1/1     Running   0          12m
kube-system    kube-scheduler-longinus-master-1            1/1     Running   0          13m
```

##### 5.3.4.2 查看指定namespace中的pod状态

```
root@longinus-master-1:~# kubectl get pod -n kube-system
NAME                                        READY   STATUS    RESTARTS   AGE
coredns-5dd5756b68-pqfhw                    1/1     Running   0          18m
coredns-5dd5756b68-vjd7k                    1/1     Running   0          18m
etcd-longinus-master-1                      1/1     Running   0          18m
kube-apiserver-longinus-master-1            1/1     Running   0          18m
kube-controller-manager-longinus-master-1   1/1     Running   0          18m
kube-proxy-j4bkt                            1/1     Running   0          18m
kube-scheduler-longinus-master-1            1/1     Running   0          18m
```

##### 5.3.4.3 查看主节点状态

```
root@longinus-master-1:~# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE   ERROR
scheduler            Healthy   ok        
controller-manager   Healthy   ok        
etcd-0               Healthy   ok    
```

注意主节点应该不存在任何`ERROR`,组件状态都应为`Healthy`

##### 5.3.4.4 查看节点列表

```
root@longinus-master-1:~# kubectl get node
NAME                STATUS   ROLES           AGE   VERSION
longinus-master-1   Ready    control-plane   19m   v1.28.2
```

## PART6. 其他节点加入集群

### 6.1 创建一个24小时内有效的token

```
root@longinus-master-1:~# kubeadm token create --ttl 24h --print-join-command
kubeadm join kubeapi.longinus.com:6443 --token u7xxyg.nxampvy3m22ifuf5 --discovery-token-ca-cert-hash sha256:cad8ad0e1f0553a6989cc9a9a73ffee219b714fd84ed6c1ce68f768ff18b733f 
```

其中:

- `--ttl`:指定token的有效期为24小时
- `--print-join-command`:打印加入集群的命令

### 6.2 worker节点加入

在`longinus-node-1`、`longinus-node-2`、`longinus-node-3`上执行如下命令:

```
root@longinus-node-1:~# kubeadm join kubeapi.longinus.com:6443 --token u7xxyg.nxampvy3m22ifuf5 --discovery-token-ca-cert-hash sha256:cad8ad0e1f0553a6989cc9a9a73ffee219b714fd84ed6c1ce68f768ff18b733f
```

执行结果如下:

```
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

注:若需加入master节点,执行如下命令即可:

```
kubeadm join kubeapi.longinus.com:6443 --token u7xxyg.nxampvy3m22ifuf5 --discovery-token-ca-cert-hash sha256:cad8ad0e1f0553a6989cc9a9a73ffee219b714fd84ed6c1ce68f768ff18b733f --control-plane
```

### 6.3 查看节点列表

```
root@longinus-master-1:~# kubectl get nodes
NAME                STATUS   ROLES           AGE   VERSION
longinus-master-1   Ready    control-plane   38m   v1.28.2
longinus-node-1     Ready    <none>          11m   v1.28.2
longinus-node-2     Ready    <none>          10m   v1.28.2
longinus-node-3     Ready    <none>          10m   v1.28.2
```

## PART7. 应用Demo

### 7.1 配置容器客户端

`crictl`是遵循CRI接口规范的一个命令行工具,随`containerd.io`一起安装,常用它来检查和管理kubelet节点上的容器运行时和镜像

`crictl`下载镜像时使用的默认端点(endpoint)是`/var/run/dockershim.sock`,这个已经在k8s v1.24起被废弃,需要我们手工指定为新的端点`containerd.sock`

重新指定的工作需要通过配置`/etc/crictl.yaml`来完成:

- 配置`crictl`

```
root@longinus-master-1:~# vim /etc/crictl.yaml
root@longinus-master-1:~# cat /etc/crictl.yaml
```

```yaml
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
```

- 检查

```
root@longinus-master-1:~# crictl ps
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
cf7dfb1a959a2       ead0a4a53df89       3 hours ago         Running             coredns                   0                   aec2d9225b4af       coredns-5dd5756b68-pqfhw
7aaaec2646b00       ead0a4a53df89       3 hours ago         Running             coredns                   0                   f1c89af71f1b7       coredns-5dd5756b68-vjd7k
dd4e3e673f68c       0dc86fe0f22e6       3 hours ago         Running             kube-flannel              0                   cd03aed7dea12       kube-flannel-ds-2clkz
c7610d72422f0       01cf8d1d322dd       4 hours ago         Running             kube-proxy                0                   e02d52661f316       kube-proxy-j4bkt
143a5a9e78c5a       9ecc4287300e3       4 hours ago         Running             kube-apiserver            0                   cdcaf8463c402       kube-apiserver-longinus-master-1
2cc97f592243e       c527ad14e0cd5       4 hours ago         Running             kube-controller-manager   0                   f90326d0098d0       kube-controller-manager-longinus-master-1
79ec5df88b210       73deb9a3f7025       4 hours ago         Running             etcd                      0                   d2709cd35d815       etcd-longinus-master-1
092fccb8a6236       babc03668f18a       4 hours ago         Running             kube-scheduler            0                   02159eb6be4d3       kube-scheduler-longinus-master-1
```

### 7.2 配置简单应用

这里我们尝试使用`crictl`命令拉取一个`nginx`镜像,之后直接使用`kubectl`将`nginx`部署为`pod`,而后改为部署为3副本的`pod`,并暴露给集群外

#### 7.2.1 拉取镜像

- 拉取镜像

```
root@longinus-master-1:~# crictl pull docker.io/nginx
Image is up to date for sha256:a8758716bb6aa4d90071160d27028fe4eaee7ce8166221a97d30440c8eac2be6
```

- 检查

```
root@longinus-master-1:~# crictl images|grep nginx
docker.io/library/nginx                         latest              a8758716bb6aa       70.5MB
```

#### 7.2.2 编排Pod

- 部署

```
root@longinus-master-1:~# kubectl create deployment nginx --image=docker.io/library/nginx:latest
deployment.apps/nginx created
```

- 检查

```
root@longinus-master-1:~# kubectl get pod -o wide
NAME                     READY   STATUS    RESTARTS   AGE   IP           NODE              NOMINATED NODE   READINESS GATES
nginx-75446c786c-9b9nd   1/1     Running   0          50s   10.244.1.2   longinus-node-1   <none>           <none>
```

- 调整Pod的副本数量为3

```
root@longinus-master-1:~# kubectl scale --replicas=3 deployment/nginx
deployment.apps/nginx scaled
```

- 检查调整后结果

```
root@longinus-master-1:~# kubectl get pods -o wide
NAME                     READY   STATUS    RESTARTS   AGE     IP           NODE              NOMINATED NODE   READINESS GATES
nginx-75446c786c-6dnxl   1/1     Running   0          86s     10.244.3.2   longinus-node-3   <none>           <none>
nginx-75446c786c-9b9nd   1/1     Running   0          3m25s   10.244.1.2   longinus-node-1   <none>           <none>
nginx-75446c786c-n5t9k   1/1     Running   0          86s     10.244.2.2   longinus-node-2   <none>           <none>
```

- 将nginx的东西向80端口,调整为南北向对外暴露的服务

```
root@longinus-master-1:~# kubectl expose deployment nginx --port=80 --type=NodePort
service/nginx exposed
```

- 检查service

```
root@longinus-master-1:~# kubectl get service -o wide
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE    SELECTOR
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP        4h1m   <none>
nginx        NodePort    10.101.152.202   <none>        80:30788/TCP   25s    app=nginx
```

此时可以访问3个worker节点的30788端口

#### 7.2.3 尝试访问应用

##### 7.2.3.1 访问Pod的IP(东西向)

在任何一个节点上访问3个pod的IP:

```
root@longinus-node-1:~/.kube# curl 10.244.1.2
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

```
root@longinus-node-1:~/.kube# curl 10.244.2.2
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

```
root@longinus-node-1:~/.kube# curl 10.244.3.2
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

##### 7.2.3.2 访问Service(南北向)

访问`http://192.168.1.57:30788/`、`http://192.168.1.58:30788/`、`http://192.168.1.59:30788/`均可看到Nginx的首页

## PART8. 备注

若想在worker节点上也使用`kubectl`命令操作集群,需要把控制平面节点上的`~/.kube/config`,scp到worker节点上,确保该文件的权限为0600即可.

[参考文章--Kubernetes v1.28安装手册-基于Ubuntu22.04](https://www.cnblogs.com/bmwhero/p/17911418.html)